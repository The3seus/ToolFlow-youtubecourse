
# ToolFlow Protocol (TFP) â€” YouTube Tutorial Branch

**This repo accompanies my YouTube series on ToolFlow Protocol â€” itâ€™s **not** the official branch! For official updates, see [HiveForensics-AI/toolflow-tfp](https://github.com/HiveForensics-AI/toolflow-tfp).**

---

## What is This?

This is a teaching/demo version of the ToolFlow Protocol (TFP): a minimalist, JSON-first API framework for building context-aware tools (APIs) that work with LLM agents **or** humans.

Iâ€™ve simplified and annotated the code for **learning purposes** â€” perfect for following along with my tutorials!

---

## âš ï¸ Disclaimer

* **NOT the official branch**: This repo is meant for YouTube viewers, experiments, and learning.
* **Production use?** Grab the [official repo here](https://github.com/HiveForensics-AI/toolflow-tfp) instead.

---

## ğŸ—ºï¸ Architecture Diagram

![ToolFlow Protocol Architecture](toolflow.png)

---

## ğŸš€ Features

* **Simple JSON protocol:** `/tfp/tools` for discovery, `/tfp/invoke` for tool calls
* **Type-safe:** Zod validation + full TypeScript types
* **LLM and RAG ready:** OpenAI, Ollama, and local vector-store RAG pipelines
* **Extensible:** Drop-in `registerTool()` for new handlers
* **Easy config:** `.env` for environment variables
* **CLI-friendly:** Works instantly with `curl` or any HTTP client

---

## ğŸ Quick Start

```bash
# 1ï¸âƒ£ Clone this repo
git clone https://github.com/The3seus/ToolFlow-youtubecourse.git
cd ToolFlow-youtubecourse

# 2ï¸âƒ£ Install dependencies
npm install

# 3ï¸âƒ£ Setup environment variables
cp .env.example .env
# Then edit .env with your OpenAI key, Ollama base URL, etc.

# 4ï¸âƒ£ Launch in dev mode (hot reload)
npm run dev
# â†’ Open http://localhost:3000

# 5ï¸âƒ£ Try out the API!
curl http://localhost:3000/tfp/tools

curl -X POST http://localhost:3000/tfp/invoke \
     -H "Content-Type: application/json"      \
     -d '{
           "toolId": "echo.v1",
           "input": { "text": "Hello from YouTube!" }
         }'
```

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ .env.example           # Environment variable template
â”œâ”€â”€ package.json           # Scripts & dependencies
â”œâ”€â”€ tsconfig.json          # TypeScript config
â”œâ”€â”€ README.md              # â† this file
â””â”€â”€ src/
    â”œâ”€â”€ protocol/
    â”‚   â””â”€â”€ types.ts       # Zod schemas & types
    â”œâ”€â”€ tools/
    â”‚   â”œâ”€â”€ echo.ts        # Example â€œecho.v1â€ tool
    â”‚   â”œâ”€â”€ llmChat.ts     # LLM chat via OpenAI or Ollama
    â”‚   â””â”€â”€ ragSearch.ts   # RAG tool (vector store)
    â”œâ”€â”€ llm/
    â”‚   â”œâ”€â”€ openaiAdapter.ts
    â”‚   â””â”€â”€ ollamaAdapter.ts
    â”œâ”€â”€ rag/
    â”‚   â””â”€â”€ vectorStore.ts
    â”œâ”€â”€ server.ts          # Express app with /tfp endpoints
    â””â”€â”€ index.ts           # App bootstrap
```

---

## ğŸŒ± Environment Setup

1. **Copy & edit environment variables:**

   ```bash
   cp .env.example .env
   # Fill in OpenAI API key, Ollama base URL, etc.
   ```

2. **Main .env fields:**

   ```
   OPENAI_API_KEY=sk-...
   OPENAI_MODEL=gpt-4o-mini
   OLLAMA_BASE_URL=http://localhost:11434
   OLLAMA_MODEL=llama3
   VECTOR_STORE_PATH=vectorStore.json
   PORT=3000
   ```

---

## ğŸ¤– Built-in Tools (API Examples)

### `echo.v1` (demo tool)

```bash
curl -X POST http://localhost:3000/tfp/invoke \
  -H "Content-Type: application/json" \
  -d '{"toolId":"echo.v1","input":{"text":"Hello world"}}'
```

---

### `llm.chat` (OpenAI or Ollama LLM)

```bash
curl -X POST http://localhost:3000/tfp/invoke \
  -H "Content-Type: application/json" \
  -d '{"toolId":"llm.chat","input":{"prompt":"Say hi!","provider":"openai"}}'
```

or (with Ollama running locally):

```bash
curl -X POST http://localhost:3000/tfp/invoke \
  -H "Content-Type: application/json" \
  -d '{"toolId":"llm.chat","input":{"prompt":"Say hi!","provider":"ollama"}}'
```

---

### `rag.search` (vector search + LLM answer)

```bash
curl -X POST http://localhost:3000/tfp/invoke \
  -H "Content-Type: application/json" \
  -d '{"toolId":"rag.search","input":{"query":"What is TFP?","provider":"openai","topK":3}}'
```

---

### `rag.addDoc` (add docs to semantic search)

```bash
curl -X POST http://localhost:3000/tfp/invoke \
  -H "Content-Type: application/json" \
  -d '{
    "toolId": "rag.addDoc",
    "input": {
      "text": "ToolFlow Protocol (TFP) is a framework for tool-based AI agents.",
      "provider": "openai"
    }
  }'
```

* Docs are embedded and added to `vectorStore.json`.
* You can now retrieve them via `rag.search`!

---

## â• Add Your Own Tool

1. **Create** `src/tools/myTool.ts`:

   ```ts
   import { z } from 'zod';
   import { ToolDescriptor } from '../protocol/types';

   export const inputSchema = z.object({
     foo: z.string(),
     bar: z.number().optional()
   });
   export const outputSchema = z.object({
     result: z.string()
   });

   export const descriptor: ToolDescriptor = {
     toolId: 'myTool.v1',
     name: 'My Tool',
     description: 'Does something awesome.',
     version: '1.0.0',
     inputSchema,
     outputSchema,
     tags: ['example','demo']
   };

   export async function handler(input: z.infer<typeof inputSchema>) {
     return { result: `You sent foo=${input.foo}` };
   }
   ```

2. **Register** it in `src/server.ts`:

   ```ts
   import { descriptor as myDesc, handler as myHandler } from './tools/myTool';
   registerTool(myDesc, myHandler);
   ```

3. **Restart** the server. It will appear in `/tfp/tools`!

---

## ğŸ”¥ Error Handling

All responses use structured JSON, including errors:

* **Unknown tool:**

  ```json
  {
    "requestId": "...",
    "toolId": "nope.v1",
    "metadata": { "timestamp": "...", "status": "error" },
    "error": { "code": "ToolNotFound", "message": "Unknown tool" }
  }
  ```
* **Validation error:**

  ```json
  {
    "requestId": "...",
    "toolId": "echo.v1",
    "metadata": { "timestamp": "...", "status": "error" },
    "error": {
      "code": "ZodError",
      "message": "Expected string, received number at path: text"
    }
  }
  ```

---

## ğŸ§ª Testing (with `curl` or Postman)

* **List tools:**
  `GET http://localhost:3000/tfp/tools`

* **Invoke a tool:**
  `POST http://localhost:3000/tfp/invoke`
  Header: `Content-Type: application/json`
  Body example:

  ```json
  {
    "toolId": "llm.chat",
    "input": { "prompt": "Hello!" }
  }
  ```

  `curl` command:

---

### ğŸ“° `headline.generator.v1` â€” Generate Catchy Headlines

Create attention-grabbing headlines from any text!
Customize the **tone** for your use caseâ€”like `"energetic"`, `"professional"`, `"playful"`, and more.

**Example:**

```bash
curl --location 'http://localhost:3000/tfp/invoke' \
  --header 'Content-Type: application/json' \
  --data '{
    "toolId": "headline.generator.v1",
    "input": {
      "text": "Our new AI platform automates workflows and boosts productivity across teams.",
      "tone": "energetic"
    }
  }'
```

**Sample Response:**

```json
{
  "requestId": "...",
  "toolId": "headline.generator.v1",
  "output": {
    "headline": "Supercharge Your Team: Discover AI-Powered Productivity for Modern Workflows!"
  },
  "metadata": {
    "status": "success",
    "timestamp": "..."
  }
}
```

* **`text`**: The source content to summarize as a headline.
* **`tone`**: (Optional) Style for the headline, e.g., `energetic`, `professional`, etc.



---

## ğŸ—ï¸ Building & Deployment

```bash
# Build production bundle
npm run build

# Start compiled server
npm start

# Or run in Docker (example)
docker build -t toolflow-tutorial:latest .
docker run -p 3000:3000 toolflow-tutorial:latest
```

---

## ğŸ”¬ Explore & Customize!

* Add new tools (see above)
* Swap out LLM adapters, add more vector stores, etc.
* Try building a simple front-end for `/tfp/tools` and `/tfp/invoke`

---

## ğŸ“œ License

Apache 2.0 â€” see [LICENSE](LICENSE)

---

**Happy hacking, and see you on YouTube! ğŸš€**

---

Let me know if you want a more â€œYouTube-personalityâ€ tone, or any extra tips/FAQ for your viewers!
